{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4957719d-decd-46b8-b2d0-04274ea92e81",
   "metadata": {},
   "source": [
    "Problem 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ca4df-bb10-4fe8-b9db-457b198ba707",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "import pandas as pd\n",
    "\n",
    "iris = load_iris()\n",
    "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
    "y = pd.Series(iris.target, name=\"target\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "results = []\n",
    "for max_depth in range(1, 6):\n",
    "\n",
    "    clf = DecisionTreeClassifier(\n",
    "        max_depth=max_depth,\n",
    "        min_samples_leaf=2,\n",
    "        min_samples_split=5,\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)      \n",
    "    y_pred = clf.predict(X_test)\n",
    "    \n",
    "\n",
    "    precision = precision_score(y_test, y_pred, average=\"macro\")\n",
    "    recall = recall_score(y_test, y_pred, average=\"macro\")\n",
    "    f1 = f1_score(y_test, y_pred, average=\"macro\")\n",
    "    \n",
    "    results.append({\n",
    "        \"max_depth\": max_depth,\n",
    "        \"precision\": round(precision, 2),\n",
    "        \"recall\": round(recall, 2),\n",
    "        \"f1\": round(f1, 2)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f0048c1b-2322-46af-b367-1644c4de483e",
   "metadata": {},
   "source": [
    "1.\n",
    "Highest Recall: Depth = 3 (Recall = 0.98).\n",
    "Why: The tree depth of 3 allows sufficient complexity to capture class boundaries without overfitting, reducing false negatives.\n",
    "\n",
    "Lowest Precision: Depth = 1 (Precision = 0.70).\n",
    "Why: A shallow tree (depth=1) underfits, leading to many false positives due to limited splits.\n",
    "\n",
    "Best F1 Score: Depth = 3 (F1 = 0.97).\n",
    "Why: F1 balances precision and recall, peaking when both are optimized.\n",
    "\n",
    "2.\n",
    "Micro:  Calculate global TP FP、FN， Calculate the indicators again. Suitable for situations with imbalanced categories.\n",
    "\n",
    "Macro:  Calculate the indicators for each category and then take the average. Suitable for balancing the importance of categories.\n",
    "\n",
    "Weighted:  Similar to Macro, but weighted by category weight (sample size). Suitable for imbalanced categories but wishing to focus on the main categories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd61b037-fcb5-4a99-bba1-764166687950",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a660ec5f-b2b4-4bf6-aa2b-179410894a7e",
   "metadata": {},
   "source": [
    "Problem 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca439550-50d3-45d3-a4b1-5121c01c795b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature of first spilt: Uniformity of Cell Size, Threshold: 2.50\n",
      "Gini parent: 0.455\n",
      "Entropy parent: 0.934\n",
      "Misclassification parent: 0.350\n",
      "Information gain: 0.589\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from scipy.stats import entropy\n",
    "\n",
    "# 定义列名\n",
    "columns = [\n",
    "    'id', 'Clump Thickness', 'Uniformity of Cell Size', 'Uniformity of Cell Shape',\n",
    "    'Marginal Adhesion', 'Single Epithelial Cell Size', 'Bare Nuclei',\n",
    "    'Bland Chromatin', 'Normal Nucleoli', 'Mitoses', 'Class'\n",
    "]\n",
    "\n",
    "# 加载数据，处理缺失值（缺失值标记为'?'）\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/breast-cancer-wisconsin.data',\n",
    "    names=columns,\n",
    "    na_values='?'\n",
    ")\n",
    "\n",
    "# 删除包含缺失值的行\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# 将目标变量转换为二进制（0表示良性，1表示恶性）\n",
    "df['Class'] = df['Class'].replace({2: 0, 4: 1})\n",
    "\n",
    "# 分离特征和目标\n",
    "X = df.drop(['id', 'Class'], axis=1)\n",
    "y = df['Class']\n",
    "\n",
    "clf = DecisionTreeClassifier(\n",
    "    max_depth=2,\n",
    "    min_samples_leaf=2,\n",
    "    min_samples_split=5,\n",
    "    criterion='gini',\n",
    "    random_state=42\n",
    ")\n",
    "clf.fit(X, y)\n",
    "\n",
    "# 根节点是索引0\n",
    "feature_idx = clf.tree_.feature[0]\n",
    "threshold = clf.tree_.threshold[0]\n",
    "feature_name = X.columns[feature_idx]\n",
    "print(f\"Feature of first spilt: {feature_name}, Threshold: {threshold:.2f}\")\n",
    "\n",
    "# 父节点样本数\n",
    "n_samples = clf.tree_.n_node_samples[0]\n",
    "\n",
    "# 父节点正类比例\n",
    "p_parent = y.mean()\n",
    "\n",
    "# 基尼系数\n",
    "gini_parent = 2 * p_parent * (1 - p_parent)\n",
    "\n",
    "# 熵\n",
    "entropy_parent = entropy([p_parent, 1 - p_parent], base=2)\n",
    "\n",
    "# 误分类错误率\n",
    "misclassification_parent = 1 - max(p_parent, 1 - p_parent)\n",
    "\n",
    "print(f\"Gini parent: {gini_parent:.3f}\")\n",
    "print(f\"Entropy parent: {entropy_parent:.3f}\")\n",
    "print(f\"Misclassification parent: {misclassification_parent:.3f}\")\n",
    "\n",
    "# 划分左右子节点\n",
    "left_mask = X.iloc[:, feature_idx] <= threshold\n",
    "y_left = y[left_mask]\n",
    "y_right = y[~left_mask]\n",
    "\n",
    "# 左子节点指标\n",
    "p_left = y_left.mean()\n",
    "gini_left = 2 * p_left * (1 - p_left)\n",
    "entropy_left = entropy([p_left, 1 - p_left], base=2)\n",
    "misclassification_left = 1 - max(p_left, 1 - p_left)\n",
    "\n",
    "# 右子节点指标\n",
    "p_right = y_right.mean()\n",
    "gini_right = 2 * p_right * (1 - p_right)\n",
    "entropy_right = entropy([p_right, 1 - p_right], base=2)\n",
    "misclassification_right = 1 - max(p_right, 1 - p_right)\n",
    "\n",
    "# 加权平均不纯度\n",
    "n_left = len(y_left)\n",
    "n_right = len(y_right)\n",
    "weighted_gini = (n_left / n_samples) * gini_left + (n_right / n_samples) * gini_right\n",
    "weighted_entropy = (n_left / n_samples) * entropy_left + (n_right / n_samples) * entropy_right\n",
    "weighted_misclassification = (n_left / n_samples) * misclassification_left + (n_right / n_samples) * misclassification_right\n",
    "\n",
    "# 信息增益（基于熵）\n",
    "information_gain = entropy_parent - weighted_entropy\n",
    "\n",
    "print(f\"Information gain: {information_gain:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60cfce9-ce18-4bf1-af50-f49d531bc99f",
   "metadata": {},
   "source": [
    "1. Information gain:\n",
    "The information gain is 0.589, indicating that the splitting of this feature significantly reduces uncertainty.\n",
    "\n",
    "2. Selected features and thresholds:\n",
    "The characteristic of the first split is Uniformity of Cell Size, with a threshold of 2.50.\n",
    "\n",
    "3. Value determines the decision boundary：\n",
    "The decision boundary value is threshold: 2.50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39e31708-de67-427a-99bf-1a67f96138a0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8ddad479-60aa-472a-b842-7294456d0ba4",
   "metadata": {},
   "source": [
    "Problem 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "00ec7c75-8800-4caa-a288-e823a69d363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Precision    Recall  F1 Score  False Positives (FP)  \\\n",
      "0  Original Data   1.000000  0.781250  0.877193                     0   \n",
      "1         PCA-1D   0.865672  0.906250  0.885496                     9   \n",
      "2         PCA-2D   0.948276  0.859375  0.901639                     3   \n",
      "\n",
      "   True Positives (TP)  False Positive Rate (FPR)  True Positive Rate (TPR)  \n",
      "0                   50                   0.000000                  0.781250  \n",
      "1                   58                   0.084112                  0.906250  \n",
      "2                   55                   0.028037                  0.859375  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "# 加载数据\n",
    "columns = ['id', 'diagnosis'] + [f'feature_{i}' for i in range(1, 31)]\n",
    "df = pd.read_csv(\n",
    "    'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data',\n",
    "    header=None,\n",
    "    names=columns\n",
    ")\n",
    "\n",
    "# 目标变量转换（M=1, B=0）\n",
    "df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "# 分离特征和目标变量\n",
    "X = df.drop(['id', 'diagnosis'], axis=1)\n",
    "y = df['diagnosis']\n",
    "\n",
    "# 标准化特征\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "\n",
    "def train_and_evaluate(X_data, y_data, model_name=\"Model\"):\n",
    "    \"\"\" 训练决策树并计算 Precision, Recall, F1, FP, TP, FPR, TPR \"\"\"\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_data, y_data, test_size=0.3, stratify=y, random_state=42\n",
    "    )\n",
    "\n",
    "    # 训练决策树\n",
    "    clf = DecisionTreeClassifier(\n",
    "        max_depth=3, min_samples_leaf=2, min_samples_split=5, criterion='gini', random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # 预测\n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    # 计算指标\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    recall = recall_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "    # 计算混淆矩阵\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    fpr = fp / (fp + tn)  # 假阳性率\n",
    "    tpr = tp / (tp + fn)  # 真阳性率\n",
    "\n",
    "    return {\n",
    "        \"Model\": model_name,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1 Score\": f1,\n",
    "        \"False Positives (FP)\": fp,\n",
    "        \"True Positives (TP)\": tp,\n",
    "        \"False Positive Rate (FPR)\": fpr,\n",
    "        \"True Positive Rate (TPR)\": tpr\n",
    "    }\n",
    "\n",
    "\n",
    "# 训练和评估不同模型\n",
    "results = []\n",
    "results.append(train_and_evaluate(X_scaled, y, \"Original Data\"))  # 原始数据\n",
    "\n",
    "# PCA 1D\n",
    "pca1 = PCA(n_components=1)\n",
    "X_pca1 = pca1.fit_transform(X_scaled)\n",
    "results.append(train_and_evaluate(X_pca1, y, \"PCA-1D\"))\n",
    "\n",
    "# PCA 2D\n",
    "pca2 = PCA(n_components=2)\n",
    "X_pca2 = pca2.fit_transform(X_scaled)\n",
    "results.append(train_and_evaluate(X_pca2, y, \"PCA-2D\"))\n",
    "\n",
    "# 以 Pandas DataFrame 格式输出结果\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0431b4c4-fa29-4553-889b-28b24d5a7094",
   "metadata": {},
   "source": [
    "1. F1 Score, Precision, and Recall:\n",
    "Original Data:\n",
    "Precision: 1.000, Recall: 0.781, F1 Score: 0.877\n",
    "PCA-1D:\n",
    "Precision: 0.866, Recall: 0.906, F1 Score: 0.885\n",
    "PCA-2D:\n",
    "Precision: 0.948, Recall: 0.859, F1 Score: 0.902\n",
    "\n",
    "\n",
    "3. Confusion Matrix:\n",
    "Original Data: FP = 0, TP = 50, FPR = 0.000, TPR = 0.781\n",
    "PCA-1D: FP = 9, TP = 58, FPR = 0.084, TPR = 0.906\n",
    "PCA-2D: FP = 3, TP = 55, FPR = 0.028, TPR = 0.859\n",
    "\n",
    "\n",
    "4. Is Continuous Data Beneficial?\n",
    "Continuous Data is beneficial for minimizing false positives, but PCA improves recall, especially in detecting malignant cases.\n",
    "\n",
    "If precision is more important, use Original Data; if recall is critical, use PCA."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
